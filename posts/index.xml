<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>All Posts - zqw's notes</title><link>https://uzqw.github.io/posts/</link><description>All Posts | zqw's notes</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Tue, 17 Feb 2026 10:00:00 +0800</lastBuildDate><atom:link href="https://uzqw.github.io/posts/" rel="self" type="application/rss+xml"/><item><title>Six Pitfalls Implementing HNSW: A Debug Journal from 0% to 98%+ Recall</title><link>https://uzqw.github.io/posts/260216-hnsw-implementation/</link><pubDate>Mon, 16 Feb 2026 10:00:00 +0800</pubDate><author>Paul</author><guid>https://uzqw.github.io/posts/260216-hnsw-implementation/</guid><description><![CDATA[<p>While implementing the HNSW algorithm for my vector database project <a href="https://github.com/uzqw/vex" target="_blank" rel="noopener noreffer ">vex</a> (initial code AI-assisted, debugging and optimization done by hand), I ran my first Recall test — <strong>every single metric was 0%</strong>. Not 50%, not 30%. Zero. The HNSW results and BruteForce results had absolutely no overlap. This post documents the entire journey from 0% to 98%+ recall, cross-referencing each bug with the original HNSW paper.</p>]]></description></item><item><title>Temporal Single-Node Reliability Deep Dive: How It Survives Server Crashes, Network Failures, and Rollback Errors</title><link>https://uzqw.github.io/posts/260213-temporal-reliability/</link><pubDate>Fri, 13 Feb 2026 10:00:00 +0800</pubDate><author>Paul</author><guid>https://uzqw.github.io/posts/260213-temporal-reliability/</guid><description><![CDATA[<p>Writing business logic with Temporal is pure joy — &ldquo;code like a single-machine program, get distributed fault tolerance for free.&rdquo; But after the honeymoon phase, a nagging question remains: <strong>Is it really reliable?</strong> What if the Server crashes? What about network partitions? What if the rollback itself fails? This article tears apart these questions from the ground up.</p>]]></description></item><item><title>Dive into K8s 02: Cilium Replaces Flannel — Goodbye VXLAN Tax, Hello eBPF Native Networking</title><link>https://uzqw.github.io/posts/251210-dive-into-k8s-02/</link><pubDate>Wed, 10 Dec 2025 17:38:00 +0800</pubDate><author>Paul</author><guid>https://uzqw.github.io/posts/251210-dive-into-k8s-02/</guid><description><![CDATA[<p>Last post, we hand-rolled Flannel in Kind cluster and witnessed the 50-byte per-packet VXLAN encapsulation overhead via tcpdump. This time, we replace Flannel with Cilium, leveraging eBPF to completely eliminate this &ldquo;network tax&rdquo;, and use Hubble for traffic visualization.</p>]]></description></item><item><title>Dive into K8s: Hand-Rolling CNI with Kind, From CrashLoop to VXLAN Packet Capture</title><link>https://uzqw.github.io/posts/251207-dive-into-k8s-01/</link><pubDate>Sun, 07 Dec 2025 17:38:00 +0800</pubDate><author>Paul</author><guid>https://uzqw.github.io/posts/251207-dive-into-k8s-01/</guid><description><![CDATA[<p>This weekend, I built a &ldquo;networkless&rdquo; cluster from scratch on Manjaro using <code>kind</code>, manually debugged missing kernel modules and CNI plugins, and finally witnessed the VXLAN encapsulation process with <code>tcpdump</code>. This post documents the entire journey.</p>]]></description></item><item><title>Temporal vs K8s Controller: Declarative vs Imperative Control Plane Paradigms</title><link>https://uzqw.github.io/posts/250715-temporal-vs-k8s-controller/</link><pubDate>Tue, 15 Jul 2025 10:00:00 +0800</pubDate><author>Paul</author><guid>https://uzqw.github.io/posts/250715-temporal-vs-k8s-controller/</guid><description><![CDATA[<p>K8s Controllers and Temporal are both &ldquo;control plane&rdquo; technologies, but they represent fundamentally different design philosophies. This post deep-dives into their core differences and explores how to combine them in AI Infra platforms.</p>]]></description></item><item><title>Temporal in Practice: From Saga Pattern to Self-Healing Distributed Transactions</title><link>https://uzqw.github.io/posts/250710-temporal-saga/</link><pubDate>Thu, 10 Jul 2025 10:00:00 +0800</pubDate><author>Paul</author><guid>https://uzqw.github.io/posts/250710-temporal-saga/</guid><description>&lt;p>While building a trading system backend, I used Temporal to solve the classic distributed transaction consistency problem. This post breaks down Temporal&amp;rsquo;s core mechanisms and explores its applications in AI task scheduling.&lt;/p></description></item><item><title>Go Lock Performance Traps: From sync.Mutex to Spinlock Tuning</title><link>https://uzqw.github.io/posts/250402-golang-interview/</link><pubDate>Wed, 02 Apr 2025 18:37:00 +0800</pubDate><author>Paul</author><guid>https://uzqw.github.io/posts/250402-golang-interview/</guid><description>&lt;p>A seemingly simple concurrent Map access, performance crashes under high concurrency. pprof shows 90% of time spent in &lt;code>sync.Mutex.Lock&lt;/code>. This post documents the investigation and performance comparison of different lock strategies.&lt;/p></description></item><item><title>Custom Linux Distro: From Yocto Build to 60% Boot Time Optimization</title><link>https://uzqw.github.io/posts/230615-linux-boot-optimization/</link><pubDate>Thu, 15 Jun 2023 10:00:00 +0800</pubDate><author>Paul</author><guid>https://uzqw.github.io/posts/230615-linux-boot-optimization/</guid><description>&lt;p>A key user experience metric for NAS products: how long from pressing power to accessing shared folders? Our target was 30 seconds, but the original took 75 seconds. This post documents the complete journey from 75s to 30s.&lt;/p></description></item><item><title>Go Daemon Development: Graceful SIGTERM Handling and Config Hot-Reload</title><link>https://uzqw.github.io/posts/230310-go-daemon-dev/</link><pubDate>Fri, 10 Mar 2023 10:00:00 +0800</pubDate><author>Paul</author><guid>https://uzqw.github.io/posts/230310-go-daemon-dev/</guid><description>&lt;p>For a NAS product, I needed to develop a 24/7 system daemon for disk health monitoring and scheduled task management. This post covers the key technical aspects: signal handling, graceful shutdown, and config hot-reload.&lt;/p></description></item><item><title>NAS Disk Health Monitoring: From smartctl to Custom Agent</title><link>https://uzqw.github.io/posts/220815-disk-health-agent/</link><pubDate>Mon, 15 Aug 2022 10:00:00 +0800</pubDate><author>Paul</author><guid>https://uzqw.github.io/posts/220815-disk-health-agent/</guid><description>&lt;p>A NAS&amp;rsquo;s most important job is protecting user data. Disk failures often have warning signs, and SMART technology can detect problems early. This post documents how to build a disk health monitoring agent.&lt;/p></description></item></channel></rss>